---
title: "Data wrangle Chl"
author: "Tim van der Stap & Julian Gan"
date: "5/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(readxl)
library(here)
library(parsedate)
library(googledrive)
library(uuid)
```

The raw data is stored on GoogleDrive. The first step is to download the file from Google Drive to a local folder on your computer's hard drive. 

``` {r file_download, eval = FALSE}
# Make sure your folder path exists already (e.g. ./chlorophyll/raw_data)
drive_download("https://drive.google.com/file/d/131MbPdsbAoggip5SVxkGbkyWXX4r5U-V/view", 
               path = here::here("chlorophyll", "raw_data", 
                                 "IYSchl_Hunt&Pakhomov.xlsx"),
               overwrite = TRUE)
```

## EventCore 

Next, we read in the Excel sheet that we are working with. The reported date / time has to be concatenated to a column `eventDate`. This column will need to get formatted to UTC in the ISO8601 extended format.

To wrangle the data into a tidy format that follows the Darwin Core Archive (DwC-A) standards and hierarchical structure, it's important to determine how many layers of `eventID` (and, consequently, `parentEventID`) the data has. In our dataset, the data has three different types of `eventID`, in order of descending hierarchy: "cruise", "station", "ndepth" - the depth at which the chlorophyll and phaeopigment concentration was measured, and "sample" - so that we create a unique eventID for each parameter measurement at a specific depth. For each of these layers, a data frame is created with unique eventIDs, associated parentEventIDs, and associated unique characteristics. In our data, we join the date to the station level, as all station-specific data was collected on a day. If data at a station was collected over multiple days, the date would be joined to a different layer. `eventID` follows a semantic model that describes the hierarchical nature of each event type.

``` {r data wrangle, eval = FALSE}
chl <- read_excel(here("chlorophyll", "raw_data", 
                       "IYSchl_Hunt&Pakhomov.xlsx"), 
                  sheet = "Chl data") %>% 
  # Convert the standalone time value (UTC+12) into ISO8601 extended format
  mutate(Time = format(Time, format = "%H:%M:%S"),
         dateTime = format_iso_8601(as.POSIXct(paste(Date, Time), 
                                               format="%Y-%m-%d %H:%M:%S", 
                                               tz="Asia/Kamchatka")),
         dateTime = str_replace(dateTime, "\\+00:00", "Z"),
         cruise = "GoA2019",
         station = paste(cruise, Station, sep="_Stn"),
         cast = paste(station, "cast1", sep=":"),
         ndepth = paste(station, Depth, sep=":niskin:"),
         sample = paste(ndepth, "T", sep=":"),
         sample = paste(sample, Tube, sep="")
  )
```

Each type of record in the Event core (in our case, cruise, station, cast, sample and subsample) has different properties (columns) associated with it, but they ultimately all get nested under a single file. Therefore, separate data frame for each type of Event record are created; this allows us to populate rows with distinct records of each type and fill the relevant columns with data. Once all the event type data frames are created, they can be concatenated to form the Event core. 

The "station" event category exists to tie casts and trawl data together. eventDate, decimalLatitude and decimalLongitude are attributes associated with the "cast" record type, because sampling times and cast and trawl coordinates differ amongst datasets

Finally, these tables get joined, and re-ordered based on `eventID`. This table gets saved locally and in the correct GoogleDrive folder. 

``` {r chl_event, eval = FALSE}
chl_cruise <- chl %>% 
  select(eventID = cruise) %>%
  distinct(eventID) %>%
  mutate(eventRemarks = "cruise")

# Join date to station
chl_station <- chl %>%
  select(eventID = station,
         parentEventID = cruise) %>% 
  distinct(eventID, .keep_all = TRUE) %>% 
  mutate(eventRemarks = "station")

chl_cast <- chl %>%
  select(eventID = cast,
         parenteventID = station,
         eventDate = dateTime,
         decimalLatitude = Latitude,
         decimalLongitude = Longitude) %>%
  distinct(eventID, .keep_all = TRUE) %>%
  mutate(eventRemarks = "cast")

chl_ndepth <- chl %>% 
  select(eventID = ndepth,
         parentEventID = cast,
         minimumDepthInMetres = Depth,
         maximumDepthInMetres = Depth) %>% 
  distinct(eventID, .keep_all = TRUE) %>% 
  mutate(eventRemarks = "sample")

chl_sample <- chl %>%
  select(eventID = sample,
         parentEventID = ndepth) %>%
  distinct(eventID, .keep_all = TRUE) %>%
  mutate(eventRemarks = "subsample")

chl_event <- bind_rows(chl_cruise, chl_station, chl_cast, chl_ndepth, chl_sample) %>%   select(eventID, parentEventID:maximumDepthInMetres, eventRemarks) %>%
  mutate(type = "Event")

# If you wish to re-order the Event Core, use the following code:
# order <- stringr::str_sort(chl_event$eventID, numeric=TRUE)
# chl_event <- chl_event[match(order, chl_event$eventID),]

# Make sure the folder path exists already (e.g. ./chlorophyll/tidy_data)
write_csv(chl_event, here("chlorophyll", "tidy_data", "chl_event.csv"))
drive_upload(here("chlorophyll", "tidy_data", "chl_event.csv"),
             path = "https://drive.google.com/drive/folders/1TqGK3ih2b7hPWUion5utDgxIEVLYRm5W",
             name = "chl_event.csv",
             overwrite = TRUE)
```

## eMoF

After creating the Event Core, we create an extended MeasurementOrFact (eMoF) table. The eMoF extension links measurement records with records in the Event core table. An occurrence table is not created as the datasets only contain abiotic data, and therefore the measurement records are linked directly to `eventID`. The first step in creating the eMoF, is adding columns with each of the different `eventID` layers (see chunk #5).

The Darwin Core Archive (DwC-A) works with "long" datatables, whereas usually data get recorded in a wide format. Therefore, the next step is to transpose the data into a long format, and assign the correct NERC controlled vocabulary to the variables and the measurement units. Finally, the eMoF tables gets saved locally and on GoogleDrive. 

``` {r chl_measurement, eval = FALSE}
# Select the correct eventID layer and the parameters measured
chl_measurement <- chl %>%
  select(eventID = sample, 
         Chl:Phe) %>%
  # Gather Chl and Phe into measurement types
  pivot_longer(c(`Chl`, `Phe`), names_to = "measurementType", 
               values_to = "measurementValue") %>% 
  mutate(measurementID = case_when(measurementType == "Chl" ~ paste(eventID,"chl", sep=":"),
                                   measurementType == "Phe" ~ paste(eventID,"phe", sep=":")),
         measurementType = recode(measurementType, 
                                  Chl = "chlorophyll a",
                                  Phe = "phaeopigments"),
         measurementTypeID = recode(measurementType, #URIs from NERC website
                                    "chlorophyll a" = "http://vocab.nerc.ac.uk/collection/P02/current/CPWC/",
                                    "phaeopigments" = "http://vocab.nerc.ac.uk/collection/P02/current/PHWC/"),
         measurementUnit = case_when(measurementType == "chlorophyll a" ~ "mg/m^3",
                                     measurementType == "phaeopigments" ~ "mg/m^3"),
         measurementUnitID = case_when(measurementType == "chlorophyll a" ~ "http://vocab.nerc.ac.uk/collection/P06/current/UMMC/",
                                       measurementType == "phaeopigments" ~ "http://vocab.nerc.ac.uk/collection/P06/current/UMMC/")
         
  ) %>%
  select(measurementID, measurementType, measurementTypeID,
         measurementValue, measurementUnit, measurementUnitID)

# Write up csv file and upload to GoogleDrive folder
write_csv(chl_measurement, here("chlorophyll", "tidy_data", "chl_measurement.csv"))
drive_upload(here("chlorophyll", "tidy_data", "chl_measurement.csv"),
             path = "https://drive.google.com/drive/folders/1TqGK3ih2b7hPWUion5utDgxIEVLYRm5W",
             name = "chl_eMoF.csv",
             overwrite = TRUE)
```

Optionally, a specific "Locations" table can be created as well, but this does not have to be uploaded to Google Drive.  

``` {r chl_location, eval = FALSE}
chl_loc <- chl %>% 
  distinct(Station, Region, Latitude, Longitude)
write_csv(chl_loc, here("chlorophyll", "tidy_data", "chl_location.csv"))
```

