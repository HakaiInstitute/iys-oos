---
title: "Data wrangle Chl"
author: "Tim van der Stap & Julian Gan"
date: "5/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First load required packages:
``` {r, eval=FALSE}
library(tidyverse)
library(lubridate)
library(readxl)
library(here)
library(parsedate)
library(googledrive)
library(uuid)
```

The raw data is stored on GoogleDrive. The first step is to download the file to a local folder. 

``` {r file download, eval = FALSE}
drive_download("https://drive.google.com/open?id=1y0obUAsdWeYp2nFOB596B5rGQzNyQxwN", 
               path = here::here("./datasets/GoA_2019/chlorophyll/raw_data/", 
                                 "IYSchl_Hunt&Pakhomov.xlsx"),
               overwrite = TRUE)
```

## EventCore 

The reported date / time has to get formatted to UTC in the ISO8601 extended format.

``` {r time format, eval = FALSE}
chl <- read_excel(here("./datasets/GoA_2019/chlorophyll/raw_data/", 
                       "IYSchl_Hunt&Pakhomov.xlsx"), 
                  sheet = "Chl data") %>% 
  # Convert the standalone time value (UTC+12) into ISO8601 extended format
  mutate(Time = format(Time, format = "%H:%M:%S"),
         dateTime = format_iso_8601(as.POSIXct(paste(Date, Time), 
                                               format="%Y-%m-%d %H:%M:%S", 
                                               tz="Asia/Kamchatka")),
         dateTime = str_replace(dateTime, "\\+00:00", "Z")
  ) 
```

To wrangle the data into a tidy format that follows the Darwin Core Archive (DwC-A) standards and hierarchical structure, it's important to determine how many layers of eventID (and, consequently, parentEventID) the data has. In our dataset, the data has three layers: cruise, station, and the depth at which the chlorophyll and phaeopigment concentration was measured. For each of these layers, a data frame is created with unique eventIDs, associated parentEventIDs, and associated unique characteristics. In our data, we join the date to the station level, as all station-specific data was collected on a day. If data at a station was collected over multiple days, the date would be joined to a different layer.  

``` {r data wrangle, eval = FALSE}
chl <- chl %>%
  mutate(cruise = "GoA2019",
         station = paste(cruise, Station, sep="_Stn"),
         ndepth = paste(station, Depth, sep=":"))

chl_cruise <- chl %>% 
  select(eventID = cruise) %>%
  distinct(eventID) %>%
  mutate(type = "cruise")

# Join date to station
chl_station <- chl %>%
  select(eventID = station,
         parentEventID = cruise,
         eventDate = dateTime,
         decimalLatitude = Latitude,
         decimalLongitude = Longitude) %>% 
  distinct(eventID, .keep_all = TRUE) %>% 
  mutate(type = "station")

chl_ndepth <- chl %>% 
  select(eventID = ndepth,
         parentEventID = station,
         minimumDepthInMetres = Depth,
         maximumDepthInMetres = Depth) %>% 
  distinct(eventID, .keep_all = TRUE) %>% 
  mutate(type = "sample")
```

Finally, these tables get joined, and re-ordered based on eventID, to create the Event Core. This table gets saved locally and in the correct GoogleDrive folder. 

``` {r chl event, eval = FALSE}
chl_event <- bind_rows(chl_cruise, chl_station, chl_ndepth) %>% 
  select(eventID, parentEventID:maximumDepthInMetres, type) 

order <- stringr::str_sort(chl_event$eventID, numeric=TRUE)
chl_event <- chl_event[match(order, chl_event$eventID),]

write_csv(chl_event, here("./datasets/GoA_2019/chlorophyll/raw_data/", "chl_event_fnl.csv"))
drive_upload("./datasets/GoA_2019/chlorophyll/raw_data/chl_event_fnl.csv",
             path = "https://drive.google.com/drive/folders/1TqGK3ih2b7hPWUion5utDgxIEVLYRm5W",
             name = "Chlorophyll_a_event.csv",
             overwrite = TRUE)
```

## eMoF

After creating the Event Core, we create an extended MeasurementOrFact (eMoF) table. An occurrence table is not created as the datasets only contain abiotic data. The first step in creating the eMoF, is adding columns with each of the different eventID layers (see chunk #5).

``` {r, eval = FALSE}
chl_measurement <- chl %>% 
  mutate(cruise = "GoA2019",
         station = paste(cruise, Station, sep= "_Stn"),
         ndepth = paste(station, Depth, sep=":"))
```

The Darwin Core Archive (DwC-A) works with "long" datatables, whereas usually data get recorded in a wide format. Therefore, the next step is to transpose the data into a long format, and assign the correct NERC controlled vocabulary to the variables and the measurement units. 

``` {r chl measurement wrangle, eval = FALSE}
chl_measurement <- chl_measurement %>%
  pivot_longer(c(`Chl`, `Phe`), names_to = "measurementType", 
               values_to = "measurementValue") %>% 
  mutate(measurementID = case_when(measurementType == "Chl" ~ paste(ndepth,"tube",Tube,"chl", sep="_"),
                                   measurementType == "Phe" ~ paste(ndepth,"tube",Tube,"phe", sep="_")),
         measurementType = recode(measurementType, 
                                  Chl = "Concentration of Chlorophyll a",
                                  Phe = "Concentration of Phaeopigments"),
         measurementTypeID = recode(measurementType, #URIs from NERC website
                                    "Concentration of Chlorophyll a" = "http://vocab.nerc.ac.uk/collection/P02/current/CPWC/",
                                    "Concentration of Phaeopigments" = "http://vocab.nerc.ac.uk/collection/P02/current/PHWC/"),
         measurementUnit = case_when(measurementType == "Concentration of Chlorophyll a" ~ "mg/m^3",
                                     measurementType == "Concentration of Phaeopigments" ~ "mg/m^3"),
         measurementUnitID = case_when(measurementType == "Concentration of Chlorophyll a" ~ "http://vocab.nerc.ac.uk/collection/P06/current/UMMC/",
                                       measurementType == "Concentration of Phaeopigments" ~ "http://vocab.nerc.ac.uk/collection/P06/current/UMMC/")
         
  ) %>%
  select(measurementID, measurementType, measurementTypeID,
         measurementValue, measurementUnit, measurementUnitID)
```

Finally, the eMoF tables gets saved locally and on GoogleDrive. 

```{r chl measurement, eval = FALSE}
write_csv(chl_measurement, here("./datasets/GoA_2019/chlorophyll/raw_data/", "chl_measurement_fnl.csv"))
drive_upload("./datasets/GoA_2019/chlorophyll/raw_data/chl_measurement_fnl.csv",
             path = "https://drive.google.com/drive/folders/1TqGK3ih2b7hPWUion5utDgxIEVLYRm5W",
             name = "Chlorophyll_a_measurement.csv",
             overwrite = TRUE)
```

Optionally, a specific "Locations" table can be created as well: 

``` {r location, eval = FALSE}
chl_loc <- chl %>% 
  distinct(parentEventID, Region, decimalLatitude, decimalLongitude)
write_csv(chl_loc, here("./datasets/GoA_2019/chlorophyll/raw_data/", "chl_location.csv"))
```

