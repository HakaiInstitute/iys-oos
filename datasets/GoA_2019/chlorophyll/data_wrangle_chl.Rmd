---
title: "Data wrangle Chl"
author: "Tim van der Stap & Julian Gan"
date: "5/1/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(readxl)
library(here)
library(parsedate)
library(googledrive)
library(uuid)
```

The raw data is stored on GoogleDrive. The first step is to download the file from Google Drive to a local folder on your computer's hard drive. 

``` {r file_download, eval = FALSE}
# Make sure your folder path exists already (e.g. ./chlorophyll/raw_data)
drive_download("https://drive.google.com/open?id=1y0obUAsdWeYp2nFOB596B5rGQzNyQxwN", 
               path = here::here("chlorophyll", "raw_data", 
                                 "IYSchl_Hunt&Pakhomov.xlsx"),
               overwrite = TRUE)
```

## EventCore 

Next, we read in the Excel sheet that we are working with. The reported date / time has to be concatenated to a column `eventDate`. This column will need to get formatted to UTC in the ISO8601 extended format.

``` {r time format, eval = FALSE}
chl <- read_excel(here("chlorophyll", "raw_data", 
                       "IYSchl_Hunt&Pakhomov.xlsx"), 
                  sheet = "Chl data") %>% 
  # Convert the standalone time value (UTC+12) into ISO8601 extended format
  mutate(Time = format(Time, format = "%H:%M:%S"),
         dateTime = format_iso_8601(as.POSIXct(paste(Date, Time), 
                                               format="%Y-%m-%d %H:%M:%S", 
                                               tz="Asia/Kamchatka")),
         dateTime = str_replace(dateTime, "\\+00:00", "Z")
  ) 
```

To wrangle the data into a tidy format that follows the Darwin Core Archive (DwC-A) standards and hierarchical structure, it's important to determine how many layers of `eventID` (and, consequently, `parentEventID`) the data has. In our dataset, the data has three different types of `eventID`, in order of descending hierarchy: "cruise", "station", and "ndepth" - the depth at which the chlorophyll and phaeopigment concentration was measured. For each of these layers, a data frame is created with unique eventIDs, associated parentEventIDs, and associated unique characteristics. In our data, we join the date to the station level, as all station-specific data was collected on a day. If data at a station was collected over multiple days, the date would be joined to a different layer. `eventID` follows a sematic model that describes the hierarchical nature of each event type.   

``` {r data_wrangle, eval = FALSE}
chl <- chl %>%
  mutate(cruise = "GoA2019",
         station = paste(cruise, Station, sep="_Stn"),
         ndepth = paste(station, Depth, sep=":"))
```

Each type of record in the Event core (in our case, cruise, station, sample) has different properties (columns) associated with it, but they ultimately all get nested under a single file. Therefore, separate data frame for each type of Event record are created; this allows us to populate rows with distinct records of each type and fill the relevant columns with data. Once all the event type data frames are created, they can be concatenated to form the Event core. 

Finally, these tables get joined, and re-ordered based on `eventID`. This table gets saved locally and in the correct GoogleDrive folder. 

``` {r chl_event, eval = FALSE}
chl_cruise <- chl %>% 
  select(eventID = cruise) %>%
  distinct(eventID) %>%
  mutate(type = "cruise")

# Join date to station
chl_station <- chl %>%
  select(eventID = station,
         parentEventID = cruise,
         eventDate = dateTime,
         decimalLatitude = Latitude,
         decimalLongitude = Longitude) %>% 
  distinct(eventID, .keep_all = TRUE) %>% 
  mutate(type = "station")

chl_ndepth <- chl %>% 
  select(eventID = ndepth,
         parentEventID = station,
         minimumDepthInMetres = Depth,
         maximumDepthInMetres = Depth) %>% 
  distinct(eventID, .keep_all = TRUE) %>% 
  mutate(type = "sample")

chl_event <- bind_rows(chl_cruise, chl_station, chl_ndepth) %>% 
  select(eventID, parentEventID:maximumDepthInMetres, type) 

order <- stringr::str_sort(chl_event$eventID, numeric=TRUE)
chl_event <- chl_event[match(order, chl_event$eventID),]

# Make sure the folder path exists already (e.g. ./chlorophyll/tidy_data)
write_csv(chl_event, here("chlorophyll", "tidy_data", "chl_event_fnl.csv"))
drive_upload(here("chlorophyll", "tidy_data", "chl_event_fnl.csv"),
             path = "https://drive.google.com/drive/folders/1TqGK3ih2b7hPWUion5utDgxIEVLYRm5W",
             name = "Chlorophyll_a_event.csv",
             overwrite = TRUE)
```

## eMoF

After creating the Event Core, we create an extended MeasurementOrFact (eMoF) table. The eMoF extension links measurement records with records in the Event core table. An occurrence table is not created as the datasets only contain abiotic data, and therefore the measurement records are linked directly to `eventID`. The first step in creating the eMoF, is adding columns with each of the different `eventID` layers (see chunk #5).

``` {r eventIDs, eval = FALSE}
chl_measurement <- chl %>% 
  mutate(cruise = "GoA2019",
         station = paste(cruise, Station, sep= "_Stn"),
         ndepth = paste(station, Depth, sep=":"))
```

The Darwin Core Archive (DwC-A) works with "long" datatables, whereas usually data get recorded in a wide format. Therefore, the next step is to transpose the data into a long format, and assign the correct NERC controlled vocabulary to the variables and the measurement units. Finally, the eMoF tables gets saved locally and on GoogleDrive. 

``` {r chl_measurement, eval = FALSE}
chl_measurement <- chl_measurement %>%
  pivot_longer(c(`Chl`, `Phe`), names_to = "measurementType", 
               values_to = "measurementValue") %>% 
  mutate(measurementID = case_when(measurementType == "Chl" ~ paste(ndepth,"tube",Tube,"chl", sep="_"),
                                   measurementType == "Phe" ~ paste(ndepth,"tube",Tube,"phe", sep="_")),
         measurementType = recode(measurementType, 
                                  Chl = "Concentration of Chlorophyll a",
                                  Phe = "Concentration of Phaeopigments"),
         measurementTypeID = recode(measurementType, #URIs from NERC website
                                    "Concentration of Chlorophyll a" = "http://vocab.nerc.ac.uk/collection/P02/current/CPWC/",
                                    "Concentration of Phaeopigments" = "http://vocab.nerc.ac.uk/collection/P02/current/PHWC/"),
         measurementUnit = case_when(measurementType == "Concentration of Chlorophyll a" ~ "mg/m^3",
                                     measurementType == "Concentration of Phaeopigments" ~ "mg/m^3"),
         measurementUnitID = case_when(measurementType == "Concentration of Chlorophyll a" ~ "http://vocab.nerc.ac.uk/collection/P06/current/UMMC/",
                                       measurementType == "Concentration of Phaeopigments" ~ "http://vocab.nerc.ac.uk/collection/P06/current/UMMC/")
         
  ) %>%
  select(measurementID, measurementType, measurementTypeID,
         measurementValue, measurementUnit, measurementUnitID)

write_csv(chl_measurement, here("chlorophyll","tidy_data", "chl_measurement_fnl.csv"))
drive_upload(here("chlorophyll", "tidy_data",  "chl_measurement_fnl.csv"),
             path = "https://drive.google.com/drive/folders/1TqGK3ih2b7hPWUion5utDgxIEVLYRm5W",
             name = "Chlorophyll_a_measurement.csv",
             overwrite = TRUE)
```

Optionally, a specific "Locations" table can be created as well, but this does not have to be uploaded to Google Drive.  

``` {r chl_location, eval = FALSE}
chl_loc <- chl %>% 
  distinct(Station, Region, Latitude, Longitude)
write_csv(chl_loc, here("chlorophyll", "tidy_data", "chl_location.csv"))
```

