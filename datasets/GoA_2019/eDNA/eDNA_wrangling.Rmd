---
title: "eDNA Data Standardization"
author: "Tim van der Stap"
date: "1/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(worrms)
library(obistools)
library(readxl)
library(here)
library(parsedate)
library(googledrive)
```

## Getting started

The following line only needs to be run once to download the required data sheets from Google Drive to your computer's hard drive.

```{r drive_download}
# Make sure your folder path exists already (e.g. ./eDNA/raw_data)
drive_download("https://docs.google.com/spreadsheets/d/1K3N1UICefgU2PpwifsIUiLsWrj5Tasrk9hHd9pCT-94/edit#gid=172817106",                path = here("eDNA", "raw_data", "2020_2019_IYS_GoA_filtered_eDNA_detections.xlsx"), overwrite = TRUE)

drive_download("https://docs.google.com/spreadsheets/d/1E0LGEfksHKebkX2ZcXCXMj-Elm6m-NtGY0SeIOuw-a0/edit#gid=1345125331",
               path = here("eDNA", "raw_data", "2020_2019_sample_info.xlsx"), overwrite = TRUE)
```

Read in the dataset from your local drive, and combine the data into one dataframe. The data file `2020_2019_sample_info.xlsx` contains the information as to which station the eDNA was collected at. However, no date, time or coordinates are included, these will likely have to come from the Bridgelog file that we have yet to receive. Once received, information from that Bridgelog file will be included in the metadata associated to the Event Core of eDNA standardization. 

Let's first sub-set for the eDNA data collected during the 2019 cruise. From the eDNA_sample_info datasheet, we can determine that these have been classified under Cruise_program 'IYS_GoA_2019'. Using these unique samples for 2019, subset the relevant columns in the detections datasheet, so that only those relevant to 2019 are selected. 

``` {r data wrangle, eval = FALSE}
eDNA_sample_info <- read_excel(here("eDNA", "raw_data",
                                    "2020_2019_sample_info.xlsx")) %>% 
  filter(Cruise_program == "IYS_GoA_2019") %>%
  mutate(Set = as.numeric(Set)) %>%
  filter(Set != is.na(Set))

# Create a separate dataframe for the control samples?

eDNA_detections <- read_excel(here("eDNA", "raw_data", 
                       "2020_2019_IYS_GoA_filtered_eDNA_detections.xlsx"))
eDNA_detections_2019 <- select(eDNA_detections, contains(unique(eDNA_sample_info$Sample)))
eDNA_detections_2019 <- cbind(eDNA_detections$OTU, eDNA_detections_2019)
names(eDNA_detections_2019)[1] <- "OTU"

# When creating the event core, we'll need to combine the metadata (date, time, spatial coordinates) to the sampling event using left_join().  
eDNA <- eDNA_sample_info %>%
  mutate(project = "IYS",
         cruise = paste(project, "GoA2019", sep = ":"),
         station = paste(cruise, Set, sep = "_Stn"),
         sample = paste(station, Sample, sep = ":Sample:")
  )
```

## Event Core

In the Event Core we include the metadata information pertaining to the various levels. As no metadata pertaining to the date, time and coordinates of the eDNA sampling are included in the eDNA data (only the station number), we need to include this information from the Bridgelog and match by Sample. 

``` {r pathogen_event, eval = FALSE}
eDNA_project <- eDNA %>%
  select(eventID = project) %>%
  distinct(eventID)

eDNA_cruise <- eDNA %>% 
  select(eventID = cruise,
         parentEventID = project) %>%
  distinct(eventID, .keep_all = TRUE)

eDNA_station <- pathogen %>%
  select(eventID = station,
         parentEventID = cruise) %>% 
  distinct(eventID, .keep_all = TRUE) 

# Join date and coordinates to the eDNA sampling from the bridgelog data sheet. Minimum and maximum depth of the trawl be included here as well if the information becomes available. 
eDNA_sample <- salmon %>%
  left_join(select(
    trawl,
    station,
    UTC_start,
    X,
    Y
  )) %>%
  select(eventID = trawl,
         parentEventID = station,
         eventDate = UTC_start,
         decimalLatitude = Y,
         decimalLongitude = X) %>%
  distinct(eventID, .keep_all = TRUE) %>%
  mutate(minimumDepthInMeters = " ",
         maximumDepthInMeters = " ",
         samplingProtocol = " ")

# To connect them all together: 
eDNA_event <- bind_rows(eDNA_project,
                        eDNA_cruise,
                        eDNA_station,
                        eDNA_sample) %>%
  select(eventID, parentEventID:decimalLongitude) %>%
  mutate(basisOfRecord = "MachineObservation")

# Make sure the folder path exists already (e.g. ./eDNA/tidy_data)
write_csv(eDNA_event, here("eDNA", "tidy_data", "edna_event.csv"))
drive_upload(here("eDNA", "tidy_data", "edna_event.csv"),
             path = " ",
             name = "edna_event.csv",
             overwrite = TRUE)
```

## Occurrence Core

In the dataset `eDNA_detections`, the column OTU (Operational Taxonomic Units) lists all the species (taxa) observed or tested for. The values in the columns indicate the reads in the library assigned to the appropriate OTU. Reads >= 10 indicates a positive detection. So first filter for those rows where reads have been recorded. To do this, I first use pivot_longer() function, and filter for reads > 0.

``` {r eDNA_occ, eval = FALSE}
eDNA_taxa_2019 <- pivot_longer(eDNA_detections_2019,
                               cols = `16S_Ceph_W332`:`COI_W454`,
                               names_to = "strings",
                               values_to = "reads") %>%
  filter(reads > 0)

eDNA_2019_taxa <- data.frame(scientificname = unique(eDNA_taxa_2019$OTU))
eDNA_2019_taxa$scientificname <- as.character(eDNA_2019_taxa$scientificname)

eDNA_taxa <- worrms::wm_records_names(eDNA_2019_taxa$scientificname) %>% dplyr::bind_rows()

# Use left_join to see for which records WoRMS has an entry in their database:
match <- left_join(eDNA_2019_taxa, eDNA_taxa, by = "scientificname")
no_match <- match %>% filter(is.na(AphiaID))

# There are 22 taxa for which no record exists in the WoRMS registry, or whose observation will have to be confirmed with the data provider. Once this is done the information will either have to be entered manually or if new records created in WoRMS, the code can be run again. 

eDNA_occ <- match %>%
  dplyr::rename(scientificNameID = lsid,
                eventID = " ",
                scientificName = scientificname,
                taxonRank = rank,
                taxonomicStatus = status,
                scientificNameAuthorship = authority) %>%
  mutate(occurrenceStatus = "present",
         occurrenceID = paste(eventID, ...)) %>%
  select(eventID, occurrenceID, scientificName, scientificNameID, occurrenceStatus, parentNameUsageID,
         taxonomicStatus, scientificNameAuthorship, kingdom, phylum, class, order, family, genus, taxonRank)              
```

Finally, save the Occurrence Core locally and on Google Drive: 

``` {r save_occ, eval = FALSE}
# Make sure the folder path exists already (e.g. ./eDNA/tidy_data)
write_csv(eDNA_occ, here("eDNA", "tidy_data", "eDNA_occ.csv"))
drive_upload(here("eDNA", "tidy_data", "eDNA_occ.csv"),
             path = " ",
             name = "eDNA_occ.csv",
             overwrite = TRUE)
```
